TODO:

    Read in tickers.csv before running scraper

    Paper trading

    Reddit API

    Add Neural Net into models

    "Why is it moving?" arrow on Ameritrade
        Maybe a good indicator. More web scraping that will need maintenance.

    Fill in missing data multiple ways before running each model (Multiple Imputation, MI)

    Take all steps possible in parallel with scraper while it constructs X_new.
        Collect data into one csv.
        Filter, impute, etc.
        What about when script restarts?

    Store all price info.
        This way you can choose any date range. And it would be cool and illegal.

    Why isn't "User" appearing in ticker sources?
        Done, need to check when I start running again

    Don't forget to use the scheduler when it's ready.

Questions:

    If residuals are positively correlated with y_actual, why don't we just add some term based on z-score of predicted value?

    Fill in missing data for predicted variable the same way as predictor variables?
        Do closer y-values mean that the X values were closer than they appeared?
        The imputer object has to have the same number of columns, so I would have to impute y in y_new.

    Subset before or after imputing?
        If it is a meaningful subset that demonstrates a different behavior, then you should impute after subsetting.
        But I don't know if it's meaningful w/o looking at historical data!
        Consider clustering subsets? Are they meaningful?
        And what about subsets that depend on values that need to be imputed?
        Is there a point in subsetting at all?

Questions with answers:

    Should dColumns be included in imputation? Would there be too many values missing if d calculated before imputing?
        Solution is to merge on previous date's data as new columns after split and before imputation, impute that,
        and then subtract

    Split data into test and training before any other manipulation.
        If I do multiple imputation, I'll need a way to score each one. That's why I split into train and test! YAYYYY

else:

    Visual Report Ideas
        stats by model, subset, imputation for past runs
        % of missing data before and after preprocessing, heatmap?
        Histogram of errors
        errors vs (y or yhat?)
            yhat, but if residuals are positively correlated with y_actual, why don't we just make subtract terms based on z-score of predicted value?

Use a decorator to do train, test, new = some_func(train, test, new)?






